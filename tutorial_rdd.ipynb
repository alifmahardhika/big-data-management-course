{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 32-bit ('LENOVO-nfkiuuwM')",
   "display_name": "Python 3.8.2 32-bit ('LENOVO-nfkiuuwM')",
   "metadata": {
    "interpreter": {
     "hash": "95bfffc8a73e7b9ce7e62a4fb79bbe83b0600692a798daa39700585e2260ff1d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# **Tutorial Basic Spark RDD**\n",
    "\n",
    "## Alif Mahardhika - 1706021934"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Karena saya menjalankan spark di machine windows pribadi, perlu ada konfigurasi imports tambahan:\n",
    "'''\n",
    "import findspark\n",
    "SPARK_HOME='C:\\spark\\spark-3.0.1-bin-hadoop2.7'\n",
    "findspark.init('C:/spark/spark-3.0.1-bin-hadoop2.7')\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "source": [
    "## Persiapan:\n",
    "\n",
    "Pada bagian ini sebuah spark context dan session di inisiasi. Saya juga mendefinisikan variabel header untuk menyimpan nama-nama column berdasarkan interpretasi saya."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Soal 2.1: Load data format ke RDD dan hitung jumlah user."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'], ['2', '53', 'F', 'other', '94043']]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "#Saya tidak tahu kenapa tapi kalau bagian ini dihilangkan machine menganggap sudah ada spark context yang berjalan. Kalau di stop tanpa diinisiasi juga error. Jadi saya biarkan dan kemudian di stop\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "spark.stop()\n",
    "\n",
    "#inisiasi context dan session\n",
    "sc= SparkContext('local','example')\n",
    "spark = SparkSession(sparkContext=sc)\n",
    "\n",
    "#membaca file u.user sebagai textFile\n",
    "rdd_raw = sc.textFile(\"ml-100k/u.user\")\n",
    "header = ['no', 'age', 'sex', 'occupation','unused']\n",
    "\n",
    "#dari textFile, setiap token di split berdasarkan karakter |\n",
    "rdd = rdd_raw.map(lambda x: x.split('|'))\n",
    "rdd.take(2) #print dua row teratas untuk memastikan sudah terbaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of users: 943\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Fungsi untuk memformat ke row object RDD\n",
    "def list_to_row(keys, values):\n",
    "    row_dict = dict(zip(keys, values))\n",
    "    return Row(**row_dict)\n",
    "\n",
    "# Format data menjadi format RDD, dan print jumlah user\n",
    "rdd_rows = rdd.map(lambda x: list_to_row(header, x))\n",
    "print(\"Number of users: \" +str(rdd_rows.count()))\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Soal 2.2: Buat fungsi map yang membagi user ke rentang usia 10-80 tahun buat bin sebesar 10 tahun setiap kategori."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "df = rdd.toDF(header)\n",
    "aged_80_less = df.filter(\"age <=80\")\n",
    "between_10_80 = aged_80_less.filter(\"age>10\")\n",
    "\n",
    "over_age = df.filter(\"age > 80\")\n",
    "under_age = df.filter(\"age < 10\")\n",
    "\n",
    "between_10_80.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "withColumn() got an unexpected keyword argument 'a'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-07d7bdcd5d8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m df.withColumn('age_group',\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'under_age'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'relevant_age'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: withColumn() got an unexpected keyword argument 'a'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "a =df.withColumn('age_group',\n",
    "F.when((df.age>=0)&(df.age<=10),'under_age')\\\n",
    ".when((df.age>10)&(df.age<=80),'relevant_age')\\\n",
    ".when((df.age>80),'over_age'))\\\n",
    ".drop_duplicates(subset=['age_group']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+---+-------------+------+\n| no|age|sex|   occupation|unused|\n+---+---+---+-------------+------+\n|  1| 24|  M|   technician| 85711|\n|  2| 53|  F|        other| 94043|\n|  3| 23|  M|       writer| 32067|\n|  4| 24|  M|   technician| 43537|\n|  5| 33|  F|        other| 15213|\n|  6| 42|  M|    executive| 98101|\n|  7| 57|  M|administrator| 91344|\n|  8| 36|  M|administrator| 05201|\n|  9| 29|  M|      student| 01002|\n| 10| 53|  M|       lawyer| 90703|\n| 11| 39|  F|        other| 30329|\n| 12| 28|  F|        other| 06405|\n| 13| 47|  M|     educator| 29206|\n| 14| 45|  M|    scientist| 55106|\n| 15| 49|  F|     educator| 97301|\n| 16| 21|  M|entertainment| 10309|\n| 17| 30|  M|   programmer| 06355|\n| 18| 35|  F|        other| 37212|\n| 19| 40|  M|    librarian| 02138|\n| 20| 42|  F|    homemaker| 95660|\n+---+---+---+-------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# outlier_count = spark.sparkContext.accumulator(0)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}